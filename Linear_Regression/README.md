
# Northeastern ECE Teaching Talk

## Linear Regression  

* What is a Model?   
* Linear Models     
* Linear regression and correlation 
  - The y-intercept $\beta_0$ 
  - The slope $\beta_1$    
  - The error $\varepsilon_i$    
  - Least squares estimation  
  - Fitted values and residuals   
* The Modeling process      
* Linear regression assumptions   
  - A linear relationship between outcome and predictor variables       
  - The error term $\varepsilon_i$ has the following assumptions:  
    * have mean zero; otherwise the forecasts will be systematically biased.
    * statistical independence of the errors (in particular, no correlation between consecutive errors in the case of time series data).
    * homoscedasticity (constant variance) of the errors.
    * normality of the error distribution.   
* Assessing the model fit 
  - T-statistic   
  - Coefficient of determination ($R^2)$  
  - Adjusted ($R^2)$   
  - Hypothesis testing: Is he "true" $\beta_1 \neq 0$?    
  - P-values    
  - Confidence intervals        
  - Standard error of the regression   
  - F-test     
  - Leverage    
  - Influence       
  - K-fold cross validtion   
* Assessing the regression assumptions  
  - Residual plots    
  - Fitted values versus residuals  
  - Standardized residuals versus theoretical quantiles (Q-Q plot)        
  - Standardized residuals versus leverage    
  - Fit Plots  
* Forecasting with regression  
* No y-intercept?   
* Residual Plots   
* Formula syntax   
* Non-linear transformations of predictors   
* Multiple linear regression 
* Multi-colinearity 


###  Appendix       
* Logistic regression
* Polynomial regression
* Overfitting  
* Regularization   
* Stepwise regression   
* Jarque–Bera test  
* Durbin–Watson statistic  
* Omnibus tests  
* Degrees of freedom   
* Likelihood and Negative Log Likelihood  
* AIC, BIC, Mallows's Cp  
* Dummy variables
* Interaction Terms 


